{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "hi5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils2 import load_dataset\n",
    "\n",
    "\n",
    "from testCases_v4 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)#set default  size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/255, valores: 12288\n",
      "test/255, valores: 12288\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_flatten/255.\n",
    "print(\"train/255, valores: \"+ str(len(train_x)))\n",
    "test_x = test_x_flatten/255.\n",
    "print(\"test/255, valores: \"+ str(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# FUNCION: initialize_parameters \n",
    "def initialize_parameters(n_x, n_h, n_y): \n",
    " \n",
    "    np.random.seed(1)     \n",
    "    b = 0.01\n",
    "    W1 =  np.random.randn(n_h, n_x) * b    \n",
    "    b1 =  np.zeros((n_h, 1))\n",
    "    W2 =  np.random.randn(n_y, n_h) * b   \n",
    "    b2 =  np.zeros((n_y, 1))         \n",
    "    assert(W1.shape == (n_h, n_x))     \n",
    "    assert(b1.shape == (n_h, 1))     \n",
    "    assert(W2.shape == (n_y, n_h))     \n",
    "    assert(b2.shape == (n_y, 1))          \n",
    "    parameters = {\"W1\": W1,                   \n",
    "                  \"b1\": b1,                   \n",
    "                  \"W2\": W2,                   \n",
    "                  \"b2\": b2}          \n",
    "    return parameters   \n",
    "\n",
    "#Probando la funcion \n",
    "parameters = initialize_parameters(3,2,1) \n",
    "print(\"W1 = \" + str(parameters[\"W1\"])) \n",
    "print(\"b1 = \" + str(parameters[\"b1\"])) \n",
    "print(\"W2 = \" + str(parameters[\"W2\"])) \n",
    "print(\"b2 = \" + str(parameters[\"b2\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# FUNCCION: initialize_parameters_deep \n",
    "def initialize_parameters_deep(layer_dims):     \n",
    "    np.random.seed(3)     \n",
    "    parameters = {}     \n",
    "    L = len(layer_dims)            # number of layers in the network \n",
    " \n",
    "    for l in range(1, L):         \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01     \n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))         \n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))              \n",
    "        \n",
    "    return parameters \n",
    "    \n",
    " #probando la funcion\n",
    "parameters = initialize_parameters_deep([5,4,3]) \n",
    "\n",
    "print('W1 = '+str(parameters['W1']))\n",
    "print('b1 = '+str(parameters['b1']))\n",
    "print('W2 = '+str(parameters['W2']))\n",
    "print('b2 = '+str(parameters['b2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION: linear_forward \n",
    "def linear_forward(A, W, b):          \n",
    "    Z = np.dot(W,A)+b       \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))     \n",
    "    cache = (A, W, b)          \n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "#Probando la función \n",
    "A, W, b = linear_forward_test_case() \n",
    "Z, linear_cache = linear_forward(A, W, b) \n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION: linear_activation_forward \n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":         \n",
    "        Z, linear_cache =  linear_forward(A_prev, W, b)          \n",
    "        A, activation_cache =  sigmoid(Z)         \n",
    "    elif activation == \"relu\":         \n",
    "        Z, linear_cache =   linear_forward(A_prev, W, b)     \n",
    "        A, activation_cache =   relu(Z)        \n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))     \n",
    "    cache = (linear_cache, activation_cache) \n",
    " \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "#Probando a la función \n",
    "A_prev, W, b = linear_activation_forward_test_case() \n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\") \n",
    "print(\"With sigmoid: A = \" + str(A)) \n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\") \n",
    "print(\"With ReLU: A = \" + str(A)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
      "       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
      "       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
      "       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]), 'b1': array([[ 1.38503523],\n",
      "       [-0.51962709],\n",
      "       [-0.78015214],\n",
      "       [ 0.95560959]]), 'W2': array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
      "       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
      "       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': array([[ 1.50278553],\n",
      "       [-0.59545972],\n",
      "       [ 0.52834106]]), 'W3': array([[ 0.9398248 ,  0.42628539, -0.75815703]]), 'b3': array([[-0.16236698]])}\n",
      "6\n",
      "W2\n",
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION: L_model_forward \n",
    " \n",
    "def L_model_forward(X, parameters): \n",
    "     \n",
    "    caches = []     \n",
    "    A = X     \n",
    "    L = len(parameters)  // 2\n",
    "    \n",
    "    # número de capas de la red     \n",
    "    caches = list() \n",
    " \n",
    "    # Implemente [LINEAR -> RELU]*(L-1). Agregue \"cache\" a la lista \"caches\"      \n",
    "    for l in range(1, L):         \n",
    "        A_prev = A  \n",
    "        A, cache =  linear_activation_forward(A_prev,  parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activation=\"relu\") \n",
    "        caches.append(cache)\n",
    "    # Implementa LINEAR -> SIGMOID. Agregue \"cache\" a la lista  \"caches\".          \n",
    "    AL, cache =  linear_activation_forward(A,  parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))                  \n",
    "    \n",
    "    return AL, caches \n",
    "\n",
    "\n",
    "#Probando la función \n",
    "X, parameters = L_model_forward_test_case_2hidden() \n",
    "print(parameters) \n",
    "print(len(parameters)) \n",
    "print(\"W\"+str(2)) \n",
    "AL, caches = L_model_forward(X, parameters) \n",
    "print(\"AL = \" + str(AL)) \n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost= 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "def computer_cost(AL,Y):\n",
    "    m=Y.shape[1]\n",
    "\n",
    "    cost = -(1/m) * (np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),(np.log(1-AL))))) \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "    \n",
    "    return cost\n",
    "#probando la funcion\n",
    "Y, AL = compute_cost_test_case()\n",
    "print(\"cost= \"+ str(computer_cost(AL,Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev= [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW= [[-0.10076895  1.40685096  1.64992505]]\n",
      "db= [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * (dZ@A_prev.T) \n",
    "    db = (1/m) * np.sum(dZ , axis=1, keepdims=True)\n",
    "    dA_prev = W.T @ dZ\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev,dW, db\n",
    "\n",
    "#probando la funcion\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"dA_prev= \"+ str(dA_prev))\n",
    "print(\"dW= \"+ str(dW))\n",
    "print(\"db= \"+ str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "def linear_activation_backward(dA, cache, activation): \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "#Probando la función \n",
    "dAL, linear_activation_cache = linear_activation_backward_test_case() \n",
    " \n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\") \n",
    "print (\"sigmoid:\") \n",
    "print (\"dA_prev = \"+ str(dA_prev)) \n",
    "print (\"dW = \" + str(dW)) \n",
    "print (\"db = \" + str(db) + \"\\n\") \n",
    " \n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\") \n",
    "print (\"relu:\") \n",
    "print (\"dA_prev = \"+ str(dA_prev)) \n",
    "print (\"dW = \" + str(dW)) \n",
    "print (\"db = \" + str(db)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Iniciando el backpropagation\n",
    "    dAL = - ((Y / AL) - ((1 - Y)/ (1 - AL)))\n",
    "\n",
    "    # en cada capa l (SIGMOID -> LINEAR) gradients. Entrada: \"dAL, current_cache\". Salida: \n",
    "    # \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1] \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward( dAL, current_cache, activation = \"sigmoid\" )\n",
    "\n",
    "    # Loop de l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # En cada capa l: (RELU -> LINEAR) gradients.\n",
    "        # Entrada: \"grads[\"dA\" + str(l + 1)], current_cache\". Salida: \n",
    "        #\"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l +1)] , grads[\"db\" + str(l + 1)]\n",
    "\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)],current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads\n",
    "\n",
    "#Probando la función\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # numero de capas de la red\n",
    "    for l in range(1,(L+1)):\n",
    "        parameters[\"W\" + str(l)] =(parameters[\"W\" + str(l)] - learning_rate * (grads[\"dW\" + str(l)]) )\n",
    "        parameters[\"b\" + str(l)] =(parameters[\"b\" + str(l)] - learning_rate * (grads[\"db\" + str(l)]) )\n",
    "    return parameters\n",
    "\n",
    "#Probando la función\n",
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.01101068, -0.01185047, -0.0020565 ],\n",
      "       [ 0.01486148,  0.00236716, -0.01023785],\n",
      "       [-0.00712993,  0.00625245, -0.00160513],\n",
      "       [-0.00768836, -0.00230031,  0.00745056],\n",
      "       [ 0.01976111, -0.01244123, -0.00626417],\n",
      "       [-0.00803766, -0.02419083, -0.00923792],\n",
      "       [-0.01023876,  0.01123978, -0.00131914],\n",
      "       [-0.01623285,  0.00646675, -0.00356271],\n",
      "       [-0.01743141, -0.0059665 , -0.00588594],\n",
      "       [-0.00873882,  0.00029714, -0.02248258],\n",
      "       [-0.00267762,  0.01013183,  0.00852798],\n",
      "       [ 0.01108187,  0.01119391,  0.01487543],\n",
      "       [-0.01118301,  0.00845833, -0.0186089 ],\n",
      "       [-0.00602885, -0.01914472,  0.01048148],\n",
      "       [ 0.01333738, -0.00197415,  0.01774645],\n",
      "       [-0.00674728,  0.00150617,  0.00152946],\n",
      "       [-0.01064195,  0.00437947,  0.01938978],\n",
      "       [-0.01024931,  0.00899338, -0.00154507],\n",
      "       [ 0.01769627,  0.00483788,  0.00676216],\n",
      "       [ 0.00643163,  0.00249087, -0.01395764]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 1.39166291e-02, -1.37066901e-02,  2.38563192e-03,\n",
      "         6.14077088e-03, -8.37912273e-03,  1.45063214e-03,\n",
      "         1.16788229e-02, -2.41044701e-04, -8.88657418e-03,\n",
      "        -2.91573775e-02, -9.71840503e-03, -5.91078738e-03,\n",
      "        -5.16417368e-03, -9.59996180e-03,  3.77295234e-03,\n",
      "        -5.74708420e-03, -1.09454334e-03,  6.79071600e-03,\n",
      "        -8.55437169e-03, -3.00206075e-03],\n",
      "       [ 2.15814934e-02,  8.74285723e-03, -1.29353663e-02,\n",
      "        -7.97409382e-04,  5.64485518e-03,  1.23347104e-02,\n",
      "         1.48986395e-03, -5.30582144e-03, -7.30526644e-03,\n",
      "         6.45061985e-03,  3.13060374e-03, -5.16647925e-03,\n",
      "        -1.89071666e-03, -4.16198015e-03,  7.24657658e-03,\n",
      "        -6.89960677e-03,  4.86414475e-03,  8.51518950e-03,\n",
      "         4.86249326e-03, -8.34239851e-03],\n",
      "       [ 1.34499246e-02, -6.78212679e-03,  4.26435074e-03,\n",
      "        -7.53334794e-03, -1.74411025e-02,  2.25750266e-03,\n",
      "         2.87035165e-03, -7.74409606e-04,  2.76068497e-03,\n",
      "        -6.48410888e-03, -7.37464837e-03, -1.68090099e-03,\n",
      "         1.90927681e-02,  8.14814541e-03, -5.19991754e-03,\n",
      "         5.58713205e-03, -4.78364660e-03, -4.57260787e-03,\n",
      "         8.59284008e-03, -5.25264645e-03],\n",
      "       [-1.67563463e-02, -9.06494701e-03,  8.84152062e-04,\n",
      "         1.28007821e-03,  1.24161652e-02, -7.16025798e-03,\n",
      "         7.31465736e-03,  4.25966750e-03, -1.49013772e-03,\n",
      "         8.35843902e-03,  4.92118903e-03, -8.62308487e-03,\n",
      "         1.07168393e-02, -1.22090192e-02,  5.96154331e-04,\n",
      "         2.44416199e-05,  4.24635721e-03, -7.25433480e-03,\n",
      "        -3.49433856e-04, -1.40620027e-03],\n",
      "       [ 9.97088367e-03, -7.95914693e-03,  7.27455292e-04,\n",
      "        -2.61240485e-03, -1.29804664e-02,  2.67611247e-02,\n",
      "        -7.12190272e-04, -1.48665807e-02,  1.40862696e-02,\n",
      "        -1.07058550e-02,  3.70869972e-03,  8.62832095e-03,\n",
      "        -6.48432023e-03, -4.30890055e-03, -5.40270264e-03,\n",
      "        -1.29361010e-03, -1.62246117e-02, -1.23563662e-02,\n",
      "        -1.40786442e-03,  1.03895212e-02],\n",
      "       [ 6.31744177e-03,  1.72941743e-02,  6.94052272e-03,\n",
      "        -5.11128993e-03, -1.22843406e-03, -2.03039355e-02,\n",
      "        -9.60775110e-03, -1.02035928e-02,  2.70593425e-03,\n",
      "         6.47829797e-03, -5.60373419e-03, -5.88501620e-03,\n",
      "        -1.54655582e-02, -1.27762058e-03,  2.48168027e-03,\n",
      "         4.45780959e-03, -7.82709043e-03,  1.98848968e-02,\n",
      "         1.19505834e-02, -9.52375987e-04],\n",
      "       [-5.27187779e-03, -3.21584693e-03,  1.51130372e-03,\n",
      "        -1.86277156e-04,  4.83528787e-03,  7.68965158e-03,\n",
      "         1.36624284e-02,  1.14726479e-02, -1.10229155e-03,\n",
      "         3.88250414e-03, -3.87127181e-03, -5.87220312e-03,\n",
      "         1.91082685e-02, -4.59846150e-03,  1.99073781e-02,\n",
      "        -3.49035393e-03,  2.52825091e-03,  1.08940955e-02,\n",
      "         2.39220218e-04,  3.93125281e-03]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[-0.00241385, -0.00475525, -0.00165777, -0.00649717,  0.01631383,\n",
      "        -0.00167699,  0.01722669],\n",
      "       [-0.02685109,  0.00018421,  0.00561952, -0.00293821,  0.01094653,\n",
      "         0.00639692, -0.002746  ],\n",
      "       [ 0.00435009,  0.02811878,  0.00251995,  0.00299502, -0.00439991,\n",
      "         0.00133497, -0.01289261],\n",
      "       [-0.0019829 ,  0.02457588,  0.01067216,  0.00641421,  0.01103922,\n",
      "         0.01881755,  0.00593588],\n",
      "       [ 0.02070879,  0.01069798,  0.0016652 ,  0.01719477, -0.02359215,\n",
      "        -0.00571349,  0.00265787]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W4': array([[-0.00912091, -0.00156058, -0.00638791, -0.00654415,  0.02711926]]), 'b4': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "def initialise_parametres(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # numero de capas\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) *0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "layersDim =[3, 20, 7, 5, 1]\n",
    "f = initialise_parametres(layersDim)\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1, m))\n",
    "\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.75:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "\n",
    "    print(\"Accuracy: \" + str(np.sum((p == y) / (m*1.0))))\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 1.2, num_iterations = 3000, print_cost=False):\n",
    "\n",
    "    costs = []                       \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = computer_cost(AL, Y)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "    \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Costo despues iteracion %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('costo')\n",
    "    plt.xlabel('itetaciones')\n",
    "    plt.title(\"tasa aprendizaje =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo despues iteracion 0: 0.693188\n",
      "Costo despues iteracion 100: 0.644172\n",
      "Costo despues iteracion 200: 0.643974\n",
      "Costo despues iteracion 300: 0.643974\n",
      "Costo despues iteracion 400: 0.643974\n",
      "Costo despues iteracion 500: 0.643974\n",
      "Costo despues iteracion 600: 0.643972\n",
      "Costo despues iteracion 700: 0.643974\n",
      "Costo despues iteracion 800: 0.643974\n",
      "Costo despues iteracion 900: 0.643974\n",
      "Costo despues iteracion 1000: 0.643974\n",
      "Costo despues iteracion 1100: 0.643974\n",
      "Costo despues iteracion 1200: 0.643974\n",
      "Costo despues iteracion 1300: 0.643974\n",
      "Costo despues iteracion 1400: 0.643974\n",
      "Costo despues iteracion 1500: 0.643974\n",
      "Costo despues iteracion 1600: 0.643974\n",
      "Costo despues iteracion 1700: 0.643974\n",
      "Costo despues iteracion 1800: 0.643974\n",
      "Costo despues iteracion 1900: 0.643974\n",
      "Costo despues iteracion 2000: 0.643974\n",
      "Costo despues iteracion 2100: 0.643974\n",
      "Costo despues iteracion 2200: 0.643974\n",
      "Costo despues iteracion 2300: 0.643974\n",
      "Costo despues iteracion 2400: 0.643974\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(train_x, train_y, [12288, 20, 7, 1], num_iterations = 2500, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
